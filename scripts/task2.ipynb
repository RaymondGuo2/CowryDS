{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84170132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "from task2_cleaning import exclude_february, data_segmentation\n",
    "from nlp import obtain_corpus, normalise_corpus, build_feature_matrix, get_topics_terms_weights, print_topics_udf\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073d4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/raymondguo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/raymondguo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/raymondguo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/raymondguo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raymondguo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/raymondguo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')  \n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  \n",
    "nltk.download('omw-1.4')  \n",
    "nltk.download('stopwords')          \n",
    "nltk.download('wordnet')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe1d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here', \"'s\", 'a', 'sentence', 'with', 'contractions', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Here's a sentence with contractions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa712f1",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf4b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out February months\n",
    "df_control = pd.read_excel('../data_source/CDS_25_Task2.xlsx', 'C Control')\n",
    "df_control = exclude_february(df_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3a96f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.astype of 0      2023-02-01 00:00:00\n",
      "1      2023-02-01 00:00:00\n",
      "2      2023-02-01 00:00:00\n",
      "3      2023-02-01 00:00:00\n",
      "4      2023-02-01 00:00:00\n",
      "              ...         \n",
      "262        01/06/2company3\n",
      "263        01/06/2company3\n",
      "264        01/06/2company3\n",
      "265        01/06/2company3\n",
      "266        01/06/2company3\n",
      "Name: TO_CHAR, Length: 267, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "df_pilot = pd.read_excel('../data_source/CDS_25_Task2.xlsx', 'C Pilot')\n",
    "print(df_pilot['TO_CHAR'].astype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0fc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data to convert the word company into 02\n",
    "df_pilot['TO_CHAR'] = df_pilot['TO_CHAR'].replace(to_replace='.*company.*', value='02', regex=True)\n",
    "df_pilot['TO_CHAR'] = pd.to_datetime(df_pilot['TO_CHAR'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa98385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLUMN_4</th>\n",
       "      <th>VOLT_FLAG</th>\n",
       "      <th>SURVEY_ID</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>LTR_COMMENT</th>\n",
       "      <th>PRIMARY_REASON</th>\n",
       "      <th>TO_CHAR</th>\n",
       "      <th>CONNECTION_TIME</th>\n",
       "      <th>SALES_PERSON_SAT</th>\n",
       "      <th>SALES_FRIENDLY_SAT</th>\n",
       "      <th>COMMINICATION_SAT</th>\n",
       "      <th>FIRST_BILL_SAT</th>\n",
       "      <th>AGENT_KNOWLEDGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>pilot</td>\n",
       "      <td>yes</td>\n",
       "      <td>352299145</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Customer Service,General,UK Legacy</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pilot</td>\n",
       "      <td>yes</td>\n",
       "      <td>351717614</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352217961</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>351710544</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pilot</td>\n",
       "      <td>yes</td>\n",
       "      <td>351645277</td>\n",
       "      <td>7</td>\n",
       "      <td>Prompt service</td>\n",
       "      <td>General Services,Internet,Pricing</td>\n",
       "      <td>NaT</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359879046</td>\n",
       "      <td>8</td>\n",
       "      <td>They are efficient</td>\n",
       "      <td>Customer Service,General,Processes/Journeys,Te...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>pilot</td>\n",
       "      <td>yes</td>\n",
       "      <td>361243832</td>\n",
       "      <td>7</td>\n",
       "      <td>Helpful and informative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>361219755</td>\n",
       "      <td>10</td>\n",
       "      <td>The gentleman who dealt us was so helpful and ...</td>\n",
       "      <td>Customer Service,General,Technician,UK Legacy</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>pilot</td>\n",
       "      <td>yes</td>\n",
       "      <td>359947966</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>pilot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>361221707</td>\n",
       "      <td>10</td>\n",
       "      <td>Really easy to get what deal I wanted and real...</td>\n",
       "      <td>Customer Service,General,Pricing,Processes/Jou...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    COLUMN_4 VOLT_FLAG  SURVEY_ID  SCORE  \\\n",
       "20     pilot       yes  352299145     10   \n",
       "21     pilot       yes  351717614      8   \n",
       "22     pilot       NaN  352217961      9   \n",
       "23     pilot       NaN  351710544      7   \n",
       "24     pilot       yes  351645277      7   \n",
       "..       ...       ...        ...    ...   \n",
       "262    pilot       NaN  359879046      8   \n",
       "263    pilot       yes  361243832      7   \n",
       "264    pilot       NaN  361219755     10   \n",
       "265    pilot       yes  359947966      0   \n",
       "266    pilot       NaN  361221707     10   \n",
       "\n",
       "                                           LTR_COMMENT  \\\n",
       "20                                                 NaN   \n",
       "21                                                 NaN   \n",
       "22                                                 NaN   \n",
       "23                                                 NaN   \n",
       "24                                      Prompt service   \n",
       "..                                                 ...   \n",
       "262                                 They are efficient   \n",
       "263                            Helpful and informative   \n",
       "264  The gentleman who dealt us was so helpful and ...   \n",
       "265                                                NaN   \n",
       "266  Really easy to get what deal I wanted and real...   \n",
       "\n",
       "                                        PRIMARY_REASON TO_CHAR  \\\n",
       "20                  Customer Service,General,UK Legacy     NaT   \n",
       "21                                                 NaN     NaT   \n",
       "22                                                 NaN     NaT   \n",
       "23                                                 NaN     NaT   \n",
       "24                   General Services,Internet,Pricing     NaT   \n",
       "..                                                 ...     ...   \n",
       "262  Customer Service,General,Processes/Journeys,Te...     NaT   \n",
       "263                                                NaN     NaT   \n",
       "264      Customer Service,General,Technician,UK Legacy     NaT   \n",
       "265                                                NaN     NaT   \n",
       "266  Customer Service,General,Pricing,Processes/Jou...     NaT   \n",
       "\n",
       "     CONNECTION_TIME  SALES_PERSON_SAT  SALES_FRIENDLY_SAT  COMMINICATION_SAT  \\\n",
       "20              10.0               NaN                10.0               10.0   \n",
       "21               8.0               8.0                 8.0                8.0   \n",
       "22              10.0               8.0                10.0                9.0   \n",
       "23               7.0               7.0                 7.0                7.0   \n",
       "24               7.0               7.0                 6.0                7.0   \n",
       "..               ...               ...                 ...                ...   \n",
       "262              9.0               NaN                10.0                9.0   \n",
       "263             10.0              10.0                 5.0               10.0   \n",
       "264             10.0              10.0                10.0               10.0   \n",
       "265              NaN               0.0                 NaN                NaN   \n",
       "266             10.0              10.0                10.0               10.0   \n",
       "\n",
       "     FIRST_BILL_SAT  AGENT_KNOWLEDGE  \n",
       "20             10.0             10.0  \n",
       "21              8.0              7.0  \n",
       "22              9.0              9.0  \n",
       "23              7.0              8.0  \n",
       "24              7.0              7.0  \n",
       "..              ...              ...  \n",
       "262            10.0              9.0  \n",
       "263            10.0              8.0  \n",
       "264            10.0             10.0  \n",
       "265             NaN              NaN  \n",
       "266            10.0             10.0  \n",
       "\n",
       "[247 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out February months\n",
    "df_pilot = exclude_february(df_pilot)\n",
    "df_pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6aa866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['COLUMN_4', 'VOLT_FLAG', 'SURVEY_ID', 'SCORE', 'LTR_COMMENT',\n",
      "       'PRIMARY_REASON', 'TO_CHAR', 'CONNECTION_TIME', 'SALES_PERSON_SAT',\n",
      "       'SALES_FRIENDLY_SAT', 'COMMINICATION_SAT', 'FIRST_BILL_SAT',\n",
      "       'AGENT_KNOWLEDGE'],\n",
      "      dtype='object')\n",
      "Index(['COLUMN_4', 'VOLT_FLAG', 'SURVEY_ID', 'SCORE', 'LTR_COMMENT',\n",
      "       'PRIMARY_REASON', 'TO_CHAR', 'CONNECTION_TIME', 'SALES_PERSON_SAT',\n",
      "       'SALES_FRIENDLY_SAT', 'COMMINICATION_SAT', 'FIRST_BILL_SAT',\n",
      "       'AGENT_KNOWLEDGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_pilot.columns)\n",
    "print(df_control.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafd033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets, since the columns are exactly the same\n",
    "df = pd.concat([df_control, df_pilot], ignore_index=True)\n",
    "# Segment dataset\n",
    "df_v, df_nv, df_v_control, df_nv_control, df_v_treatment, df_nv_treatment = data_segmentation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e83f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n",
      "341\n",
      "158\n",
      "177\n",
      "83\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "# Confirm all data segments are correct\n",
    "print(len(df_v))\n",
    "print(len(df_nv))\n",
    "print(len(df_v_control))\n",
    "print(len(df_nv_control))\n",
    "print(len(df_v_treatment))\n",
    "print(len(df_nv_treatment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397e77b",
   "metadata": {},
   "source": [
    "#### Topic Modelling and Theme Discovery\n",
    "\n",
    "##### Clean Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee9c5a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/raymondguo/nltk_data'\n    - '/Users/raymondguo/opt/anaconda3/envs/cowryenv/nltk_data'\n    - '/Users/raymondguo/opt/anaconda3/envs/cowryenv/share/nltk_data'\n    - '/Users/raymondguo/opt/anaconda3/envs/cowryenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m general_corpus = obtain_corpus(df)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m norm_corpus = \u001b[43mnormalise_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneral_corpus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m vectoriser, tfidf_matrix = build_feature_matrix(norm_corpus, feature_type=\u001b[33m'\u001b[39m\u001b[33mtfidf\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m total_topics = \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Careers/cowry interview/CowryDS/data_preprocessing/nlp.py:148\u001b[39m, in \u001b[36mnormalise_corpus\u001b[39m\u001b[34m(corpus, lemmatise, tokenise)\u001b[39m\n\u001b[32m    146\u001b[39m text = expand_contractions(text, CONTRACTION_MAP)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lemmatise:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     text = \u001b[43mlemmatise_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m     text = text.lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Careers/cowry interview/CowryDS/data_preprocessing/nlp.py:107\u001b[39m, in \u001b[36mlemmatise_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatise_text\u001b[39m(text):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     pos_tagged_text = \u001b[43mpos_tag_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     lemmatised_tokens = [wnl.lemmatize(word, pos_tag) \u001b[38;5;28;01mif\u001b[39;00m pos_tag \u001b[38;5;28;01melse\u001b[39;00m word \u001b[38;5;28;01mfor\u001b[39;00m word, pos_tag \u001b[38;5;129;01min\u001b[39;00m pos_tagged_text]\n\u001b[32m    109\u001b[39m     lemmatised_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(lemmatised_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Careers/cowry interview/CowryDS/data_preprocessing/nlp.py:102\u001b[39m, in \u001b[36mpos_tag_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    101\u001b[39m tokens = nltk.word_tokenize(text)\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m tagged_text = \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m tagged_lower_text = [(word.lower(), penn_to_wn_tags(tag)) \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m tagged_text]\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagged_lower_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cowryenv/lib/python3.13/site-packages/nltk/tag/__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cowryenv/lib/python3.13/site-packages/nltk/tag/__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cowryenv/lib/python3.13/site-packages/nltk/tag/perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cowryenv/lib/python3.13/site-packages/nltk/tag/perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/cowryenv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/Users/raymondguo/nltk_data'\n    - '/Users/raymondguo/opt/anaconda3/envs/cowryenv/nltk_data'\n    - '/Users/raymondguo/opt/anaconda3/envs/cowryenv/share/nltk_data'\n    - '/Users/raymondguo/opt/anaconda3/envs/cowryenv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "general_corpus = obtain_corpus(df)\n",
    "norm_corpus = normalise_corpus(general_corpus)\n",
    "vectoriser, tfidf_matrix = build_feature_matrix(norm_corpus, feature_type='tfidf')\n",
    "\n",
    "total_topics = 2\n",
    "lda = LatentDirichletAllocation(n_topics=total_topics, max_iter=100, learning_method='online', learning_offset=50., random_state=42)\n",
    "lda.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727eb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectoriser.get_feature_names()\n",
    "weights = lda.components_\n",
    "\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "print_topics_udf(topics=topics, total_topics=total_topics, num_terms=8, display_weights=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cowryenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
